---
title: "Week 1 – RAG and the Mathematics of Semantics"
date: 2025-08-13
tags: [ai, learning, diary, rag, ollama]
summary: "Discovering RAG: how it works, why it’s beautiful, and the steps we took to get it running."
---

This week I learned what RAG is, how it works, and what it’s used for.
Once I understood it, I came up with at least ten different applications to build.
I also found the concept transparent and beautiful: that there is a **mathematics of semantics**. Gorgeous.

- We installed Ollama (my GPT and I, of course).
- We chose a model.
- With Ollama running, we already had the LLM part working.
- In another pot (we’re using Python), we followed these steps:
  - We took a series of documents (for now, just text strings) and passed them to the LLM to **embed** them. For each document, it returned a vector.
  - We asked a question related to the documents and also passed it to the LLM to embed (getting the question’s vector).
  - Next, using a mathematical function, we compared the vectors and looked for which documents were closest (meaning, semantically similar) to the question. This gave us the appropriate context (or mathematically bounded) to our question.
  - That context and the original question were then used with another LLM function — the one that… answers questions.
    If everything went well, we got an answer aligned with the documents we gave it.

---

**Side notes:**

- **Ollama** because, in these early experiments, I’d rather not spend money (and don’t get confused like I did — paying for ChatGPT does **not** give you an API Key).
- **Colloquial language and lack of technical detail:** because I prefer to share and consolidate the _process_, not the tools or techniques. But I will say I use ChatGPT and Cursor (the cheap subscriptions). I occasionally check out Claude or DeepSeek as well.
- **The humor:** alive and well.
